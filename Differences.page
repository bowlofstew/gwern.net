---
title: Are Individual Differences Complex?
description: Individual human brains are more predictable and similar than they are different, reflecting low Kolmogorov complexity and implying that beta uploading may be more feasible than guessed, with suggestions on optimizing archived information.
tags: psychology, sociology, statistics, transhumanism, NN
created: 23 June 2010
status: in progress
confidence: likely
importance: 4
...

Every human is different in a myriad of ways, from their memories to their personality to their skills or knowledge to intelligence and cognitive abilities to moral and political values.
But humans are also often remarkably similar, occupying a small area of mind-space - even a chimpanzee is alien in a way that their recognizable similarities to us only emphasize, never mind something like an octopus, much less aliens or AIs.

So this raises a question: are individuals, with their differences, more or less information-theoretically complex than some generic average human brain is in total?
That is, if you somehow managed to encode an average human brain into a computer upload (I'll assume patternism here), into a certain number of bits, would you then require as many or more bits to convert that average brain into a specific person, or would you require many fewer?

This bears on some issues such as:

1. in a computer upload scenario, would it be necessary to scan every individual human brain in minute detail in order to upload them, or would it be possible to map only one brain in depth and then more coarse methods would suffice for all subsequent uploads? To mold or evolve an upload towards one specialty or another, would it be feasible to start with the generic brain and train it, or are brains so complex and idiosyncratic that one would have to start with a specific brain already close to the desired goal?
2. is cryonics (expensive and highly unlikely to work) the only way to recover from death, or would it be possible to augment poor vitrification with supplemental information like diaries to enable full revivification? Or would it be possible to be recreated *entirely* from surviving data and records, so-called "beta uploading" or "beta simulations", in some more meaningful method than "simulate all possible human brains"?

When I introspect, I do not *feel* especially complex or unique or more than the product of the inputs over my life.
I feel I am the product of a large number of inbuilt & learned mechanisms, heuristics, and memories, operating mechanistically, repeatably, and unconsciously.
Once in a great while, while reading old blog posts or reviewing old emails, I compose a long reply, only to discover that I had written one already, which is similar or even exactly the same almost down to the word, and chilled, I feel like an automaton, just another system as limited and predictable to a greater intelligence as a Sphex wasp or my cat are to me, not even an especially unique one but a mediocre result of my particular assortment of genes and mutation load and congenital defects and infections and development noise and shared environment and media consumption

One way is to ask how complex the brain could be.

## Descriptive Complexity

Working from the bottom up, we could ask how much information it takes to encode individual brains.

The [Whole Brain Emulation Roadmap](http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf) reports a number of estimates of how much storage an upload might require, which reflects the complexity of a brain at various levels of detail, in "Table 8: Storage demands (emulation only, human brain)" (pg79):

> Table 8: Storage demands (emulation only, human brain)
> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------
> Level    Model                                     # entities                                                                    Bytes per entity                                                                Memory demands (Tb)  Earliest year, \$1 million
> -------  ----------------------------------------- ---------------------------------------------------------                     ----------------------------------------------------------------------          ------------------  ----------------------------
> 1        Computational module                      100 ‐ 1,000?                                                                  ?                                                                               ?                    ?
> 2        Brain region connectivity                 10^5^ regions, 10^7^ connections                                              3? (2: byte connectivity, 1 byte weight)                                        3 ∙ 10^‐5^           Present
> 3        Analog network population model           10^8^ populations, 10^13^ connections.                                        5 (3 ‐ byte connectivity, 1 byte weight, 1 byte extra state variable)           50                   Present
> 4        Spiking neural network                    10^11^ neurons, 10^15^ connections.                                           8 (4 ‐ byte connectivity, 4 state variables)                                    8,000                2019
> 5        Electrophysiology                         10^15^ compartments x 10 state variables = 10^16^ .                           1 byte per state variable                                                       10,000               2019
> 6        Metabolome                                10^16^ compartments x 10^2^ metabolites= 10^18^ .                             1 byte per state variable                                                       10^6^                2029
> 7        Proteome                                  10^16^ compartments x 10^3^ proteins and metabolites = 10^19^.                1 byte per state variable                                                       10^7^                2034
> 8        States of protein complexes               10^16^ compartments x 10^3^ proteins x 10 states = 10^20^                     1 byte per state variable                                                       10^8^                2038
> 9        Distribution of complexes                 10^16^ compartments x 10^3^  proteins and metabolites x 100 states/locations. 1 byte per state variable                                                       10^9^                2043
> 9        Full 3D EM map (Fiala, 2002)              50x2.5x2.5 nm                                                                 1 byte per voxel, compressed.                                                   10^9^                2043
> 10       Stochastic behaviour of single molecules  10^25^ molecules                                                              31 (2 bytes molecule type, 14 bytes position, 14 bytes velocity, 1 byte state)  3.1∙10^14^           2069
> 11       Quantum                                   Either  ≈ 10^26^ atoms, or smaller number of quantum‐state carrying molecules Qbits                                                                           ?                    ?

The most likely scale is the spiking neural network but not lower levels (like individual neural compartments or molecules), which they quote at 10^{11} neurons with 10^{15} connections, at 4 bytes for the connections and 4 bytes for neural state, giving 8000 terabytes - which is quite large.
If 8000tb is anywhere close to the true complexity of oneself, beta simulations are highly unlikely to result in any meaningful continuity of self, as even if one did nothing but write in diaries every waking moment, the raw text would never come anywhere near 8000tb (typing speeds tend to top out at ~100WPM, or 2.8 billion words or ~22.4GB or 0.22tb in a lifetime).

## Bandwidth Bounds

Another approach is functional: how much information can the human brain actually store when tested?
On the other hand, estimates of human brain capacity tend to be far lower.

One often cited quantification is [Landauer 1986](http://www.cs.colorado.edu/~mozer/Teaching/syllabi/7782/readings/Landauer1986.pdf "How Much Do People Remember? Some Estimates of the Quantity of Learned Information in Long-term Memory").
Landauer tested information retention/recall using text reading, picture recognition memory, and autobiographical recall, finding comparable storage estimates for all modalities:

> Table 1: Estimates of Information Held in Human Memory
> ------------------------------------------------------
> Source of parameters         Method of Estimate                 Input Rate (b/s)   Loss Rate (b/b/s)   Total (bits)
> ---------------------------- ---------------------              ------------------ ------------------- ------------
> Concentrated reading         70-year linear accumulation        1.2                                    $1.8 \cdot 10^9$
> Picture recognition          70-year linear accumulation        2.3                                    $3.4 \cdot 10^9$
> Central values               asymptotic                         2.0                10^-9^              $2.0 \cdot 10^9$
>                              net gain over 70 years                                                    $1.4 \cdot 10^9$
> Word knowledge               semantic nets x 15 domains                                                $0.5 \cdot 10^9$

Based on the forgetting curve and man-centuries of data on [spaced repetition](/Spaced repetition), [Wozniak estimates](http://super-memory.com/articles/theory.htm "Theoretical aspects of spaced repetition in learning") that permanent long-term recall of declarative memory is limited to 200-300 flashcard items per year per daily minute of review, so in a lifetime of ~80 years and a reasonable amount of time spent on review, say 10 minutes, would top out at a maximum of $300 \cdot 10 \cdot 80=240,000$ items memorized; as each flashcard should encode a minimum fact such as a single word's definition, which is certainly less than a kilobyte of entropy, long-term memory is bounded at around 240MB.
(For comparison, see [profiles](https://www.theguardian.com/science/2017/feb/08/total-recall-the-people-who-never-forget "Total recall: the people who never forget: An extremely rare condition may transform our understanding of memory") of people with ["highly superior autobiographical memory"](!Wikipedia "Hyperthymesia"), whose most striking attribute is normal memories combined with extremely large amounts of time spent diarizing or recalling or quizzing themselves on the past.)

Aside from low recall of generic autobiographical memory, [childhood amnesia](!Wikipedia) sets in around age 3-4, resulting in almost total loss of all episodic memory prior to then, and the loss of perhaps 5% of all lifetime memories; the developmental theory is that due to limited memory capacity, the initial memories simply get overwritten by later childhood learning & experiences.
(Childhood amnesia is sometimes underestimated: many 'memories' are pseudo-memories of stories retold by relatives.)

[Working memory](!Wikipedia) is considered a bottleneck that information must pass through in order to be stored in short-term memory and then potentially into long-term memory, but it is also small and slow.
[Forward digit span](!Wikipedia) is a simple test of working memory, in which one tries to store & recall short random sequences of the integers 0-10; a normal adult forward digit span (without resorting to mnemonics or other strategies) might be around 7, and requires several seconds to store and recall; thus, digit span suggests a maximum storage of $log(10)=3.32$ bits per second, or 8.3GB over a lifetime.

A good reader will read at 200-300 words per minute; Claude Shannon estimated a single character is <1 bit; English words would weigh in at perhaps 8 bits per word (as vocabularies tend to top out at around 100,000 words, each word would be at most 16 bits) or 40 bits per second, so at 1 hour a day, a lifetime of reading would convey a maximum of 70MB.
Landauer's subjects read 180 words per minute and he estimated 0.4 bits per word for a rate of 1.2 bits per second.

Deep learning researcher [Geoffrey Hinton](https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyjogf/) has repeatedly noted (in an observation echoed by additional researchers discussing the relationship between supervised learning vs reinforcement learning vs unsupervised learning like [Yann LeCun](https://simons.berkeley.edu/talks/yann-lecun-2017-3-30 "Unsupervised Representation Learning")) that the number of synapses & neurons in the brain versus the length of our lifetime implies that much of our brain must learning generic unsupervised representations such as predictive modeling:

> The brain has about 10^14^ synapses and we only live for about 10^9^ seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception) is the only place we can get 10^5^ dimensions of constraint per second.

Since we all experience very similar perceptual worlds with shared optical illusions etc, this would appear to challenge the idea of very large individual differences.
(A particularly acute problem given that in reinforcement learning, the supervision is limited to the rewards, which provide fraction of a bit spread over thousands or millions of actions taken in very high-dimensional perceptual states each of which may be hundreds of thousands or millions of parameters themselves.)

Eric Drexler notes that current deep learning models in computer vision have now achieved near-human, or superhuman performance across a wide range of tasks requiring model sizes typically around 500MB (and trained models typically can be compressed 10x or more with little or no performance loss, due to the widely-noted massive overparameterization of deep networks); the visual cortex and brain regions related to visual processing make up a large fraction of the brain (perhaps as much as 20%), suggesting that either deep models are extremely efficient compared to the brain^[Not *that* improbable, given how incredibly bizarre and convoluted much of neurobiology is, and the severe [metabolic, biological, evolutionary, genetic, and developmental constraints](/Drug heuristics) a brain must operate under.], the brain's visual processing is powerful in a way not captured by any benchmarks, or that a relatively small number of current GPUs (eg 1000-15000) are equivalent to the brain.[^Landauer-conclusions]
Consider also [animals by neural count](!Wikipedia "List of animals by number of neurons"): other primates or cetaceans can have fairly similar neuron counts as humans, implying that much of the brain is dedicated to basic mammalian tasks like vision or motor coordination, implying that all the things we see as critically important to our sense of selves, such as our religious or political views, are supported by a small fraction of the brain indeed.

[^Landauer-conclusions]: Landauer notes something very similar in his conclusion:

    > Thus, the estimates all point toward a functional learned memory content of around a billion bits for a mature person. The consistency of the numbers is reassuring...Computer systems are now being built with many billion bit hardware memories, but are not yet nearly able to mimic the associative memory powers of our “billion” bit functional capacity. An attractive speculation from these juxtaposed observations is that the brain uses an enormous amount of extra capacity to do things that we have not yet learned how to do with computers. A number of theories of human memory have postulated the use of massive redundancy as a means for obtaining such properties as content and context addressability, sensitivity to frequency of experience, resistance to physical damage, and the like (e.g., Landauer, 1975; Hopfield, 1982; Ackley, Hinton, & Sejnowski, 1985). Possibly we should not be looking for models and mechanisms that produce storage economies (e.g., Collins & Quillian, 1972), but rather ones in which marvels are produced by profligate use of capacity.

## Predictive Complexity

A third tact is to treat the brain as a black box and take a Turing-test-like view: if a system's outputs can be mimicked or predicted reasonably well by a smaller system, then that is the real complexity.
So, how predictable are human choices and traits?

One major source of predictive power is genetics.

A whole human genome has ~3 billion base-pairs, which can be stored in 1-3GB^[The raw data from the sequencer would be much larger as it consists of many repeated overlapping sequence runs, but this doesn't reflect the actual genome's size.].
Individual human differences in genetics turn out to be fairly modest in magnitude: there are perhaps 5 million small mutations and 2500 larger structural variants compared to a reference genome ([Auton et al 2015](http://www.nature.com/nature/journal/v526/n7571/full/nature15393.html "A global reference for human genetic variation"), [Chaisson et al 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4317254/ "Resolving the complexity of the human genome using single-molecule sequencing"), [Seo et al 2016](http://www.nature.com/nature/journal/v538/n7624/full/nature20098.html "De novo assembly and phasing of a Korean human genome"), [Paten et al 2017](http://biorxiv.org/content/biorxiv/early/2017/03/14/101816.full.pdf "Genome Graphs and the Evolution of Genome Inference")), many of which are correlated or familial, so given a reference genome such as a relative's, the 1GB shrinks to a much smaller delta, ~125MB with one approach, but possibly down to the MB range with more sophisticated encoding techniques such as using genome graphs rather than reference sequences.
Just SNP genotyping (generally covering common SNPs present in >=1% of the population) will typically cover ~500,000 SNPs with 3 possible values, requiring less than 1MB.

A large fraction of individual differences can be ascribed to those small genetic differences.
(For more background on modern behavioral genetics findings, see my [genetics link bibliography](/Mistakes#genetics-links).)
Across thousands of measured traits in twin studies from blood panels of biomarkers to diseases to anthropometric traits like height to psychological traits like personality or intelligence to social outcomes like socioeconomic status, the average measured heritability predict ~50% of variance ([Polderman et al 2015](/docs/genetics/2015-polderman.pdf "Meta-analysis of the heritability of human traits based on fifty years of twin studies")), with shared family environment accounting for 17%; this is a lower bound as it omits corrections for issues like assortative mating or measurement error or genetic mosaicism (particularly common in neurons).
SNPs alone account for somewhere around 15% (see [GCTA](!Wikipedia) & [Ge et al 2016](http://biorxiv.org/content/early/2016/08/18/070177 "Phenome-wide Heritability Analysis of the UK Biobank"); same caveats).
Humans are often stated to be relatively little variation and homogeneous compared to other wild species, apparently due to loss of genetic diversity during human expansion out of Africa; presumably if much of that diversity remained, heritabilities might be even larger.
Further, there is [pervasive pleiotropy in the genetic influences](!Wikipedia "Genetic correlation"), where traits overlap and influence each other, making them more predictable; and the genetic influence is often partially responsible for longitudinal consistency in individuals' traits over their lifetimes.

TODO:
'everything is correlated'
'efficient natural language'
Death Note Anonymity
Conscientiousness and online education
Simulation inferences

    13:02 <@gwern> Xern: one thing you could try is taking huge numbers of psychology inventories and surveys, which may expose more of your values and beliefs than self-directed highly redundant journaling/writing
    13:03 <@gwern> Xern: see the small 100 approach http://lesswrong.com/lw/1ay/is_cryonics_necessary_writing_yourself_into_the/
    13:03 < feepbot> Is cryonics necessary?: Writing yourself into the future - Less Wrong
    13:17 <@gwern> anyway, for beta uploading, I would say as a general principle you want to start with your genome, then use survey instruments designed for predictive power and precise measurement of important factors, then move on to recordings of interactive material (like IRC logs!), then record writings in general, then record quotidian details like your environment
    13:18 < Xern> yeah, and for the genome, "full exome" is not enough, it should be "full genome". Many important parts, for immune system, and basically everything related to splicing, is in the intron.
    13:19 < Xern> moreover, mitochondrial DNA should be included. Not even one company is proposing that to customer right now.
    13:19 <@gwern> Xern: yes. rare variants will matter. you will want full deep sequencing. and preferably first-degree relatives too - that can correct for errors and allow some childhood/family environment prediction too
    13:21 <@gwern> Xern: and preserve tissue samples too while you're at it... I understand current whole genomes are generally based on sequencing short snippets and so run into problems in highly repetitive areas of the genome or with structural variants, so if you're doing sequencing now, it'll probably have to be redone in a few decades anyway

<!--
TODO: ping yvain, Carl Shulman
ln /r/ssc, LW
-->
